{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.kernel_spec_manager_class='environment_kernels.EnvironmentKernelSpecManager'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generating script\n",
    "def place_cube(matrix, origin, r):\n",
    "\t#dim = matrix.shape()\n",
    "\tmatrix[\n",
    "\torigin[0]-r:origin[0]+r,\n",
    "\torigin[1]-r:origin[1]+r,\n",
    "\torigin[2]-r:origin[2]+r,\n",
    "\t] = 1\n",
    "\treturn matrix\n",
    "\n",
    "def dist(x, y):\n",
    "\treturn math.sqrt(\n",
    "\t\t(x[0]-y[0])**2+\n",
    "\t\t(x[1]-y[1])**2+\n",
    "\t\t(x[2]-y[2])**2\n",
    "\t\t)\n",
    "\n",
    "def place_sphere(matrix, origin, r):\n",
    "\tfor index, x in np.ndenumerate(matrix):\n",
    "\t    if dist(index, origin) <= r:\n",
    "\t    \tmatrix[index] = 1\n",
    "\treturn matrix\n",
    "\n",
    "def random_shapes(m_size,num):\n",
    "    shapes = np.zeros((m_size,m_size,m_size,num))\n",
    "    shape_types = np.random.randint(low = 0,high = 2,size= num)\n",
    "    \n",
    "    for i in range(num):\n",
    "        r = np.random.randint(low = 3,high = np.floor(m_size/2-1))\n",
    "        org = np.random.randint(low = r + 1,high = m_size - r - 1, size = 3)\n",
    "        if shape_types[i] == 0:\n",
    "            shapes[:,:,:,i] = place_cube(shapes[:,:,:,i],org,r)\n",
    "        elif shape_types[i] == 1:\n",
    "            shapes[:,:,:,i] = place_sphere(shapes[:,:,:,i],org,r)\n",
    "            \n",
    "    return shapes, shape_types\n",
    "        \n",
    "\n",
    "def matching_shapes(m_size, num):\n",
    "\tcubes = np.zeros((m_size,m_size,m_size,num))\n",
    "\tspheres = np.zeros((m_size,m_size,m_size,num))\n",
    "\tfor i in range(num):\n",
    "\t\tr = np.random.randint(low = 3,high = np.floor(m_size/2-1))\n",
    "\t\torg = np.random.randint(low = r + 1,high = m_size - r - 1, size = 3)\n",
    "\t\tcubes[:,:,:,i] = place_cube(cubes[:,:,:,i],org,r)\n",
    "\t\tspheres[:,:,:,i] = place_sphere(spheres[:,:,:,i],org,r)\n",
    "\treturn cubes, spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py\n",
    "FC_SIZE = 1024\n",
    "DTYPE = tf.float32\n",
    "\n",
    "\n",
    "def _weight_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, DTYPE, tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "\n",
    "def _bias_variable(name, shape):\n",
    "    return tf.get_variable(name, shape, DTYPE, tf.constant_initializer(0.1, dtype=DTYPE))\n",
    "\n",
    "\n",
    "def inference(boxes, dataconfig):\n",
    "     # Input Layer\n",
    "    # input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
    "    prev_layer = boxes\n",
    "\n",
    "    in_filters = dataconfig.num_props\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        out_filters = 16\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "        prev_layer = conv1\n",
    "        in_filters = out_filters\n",
    "\n",
    "    pool1 = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "    norm1 = pool1  # tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta = 0.75, name='norm1')\n",
    "\n",
    "    prev_layer = norm1\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        out_filters = 32\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "        prev_layer = conv2\n",
    "        in_filters = out_filters\n",
    "\n",
    "    # normalize prev_layer here\n",
    "    prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        out_filters = 64\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        out_filters = 64\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        out_filters = 32\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    # normalize prev_layer here\n",
    "    prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        prev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n",
    "        weights = _weight_variable('weights', [dim, FC_SIZE])\n",
    "        biases = _bias_variable('biases', [FC_SIZE])\n",
    "        local3 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    prev_layer = local3\n",
    "\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        prev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n",
    "        weights = _weight_variable('weights', [dim, FC_SIZE])\n",
    "        biases = _bias_variable('biases', [FC_SIZE])\n",
    "        local4 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    prev_layer = local4\n",
    "\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        weights = _weight_variable('weights', [dim, dataconfig.num_classes])\n",
    "        biases = _bias_variable('biases', [dataconfig.num_classes])\n",
    "        softmax_linear = tf.add(tf.matmul(prev_layer, weights), biases, name=scope.name)\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='cross_entropy_per_example')\n",
    "\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "    Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op\n",
    "\n",
    "def train(total_loss, global_step):\n",
    "    \"\"\"Train CIFAR-10 model.\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataconfig.num_props = 32\n",
    "dataconfig.num_classes = 2\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EPOCHS_PER_DECAY\n",
    "FLAGS.batch_size = 16\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = train(loss, global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:condaEnvPy2TF1.2]",
   "language": "python",
   "name": "conda-env-condaEnvPy2TF1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
