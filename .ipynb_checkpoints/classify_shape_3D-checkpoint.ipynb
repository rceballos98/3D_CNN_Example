{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.kernel_spec_manager_class='environment_kernels.EnvironmentKernelSpecManager'\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Our application logic will be added here\n",
    "\n",
    "import math\n",
    "import sys, traceback\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generating script\n",
    "def place_cube(matrix, origin, r):\n",
    "\t#dim = matrix.shape()\n",
    "\tmatrix[\n",
    "\torigin[0]-r:origin[0]+r,\n",
    "\torigin[1]-r:origin[1]+r,\n",
    "\torigin[2]-r:origin[2]+r,\n",
    "\t] = 1\n",
    "\treturn matrix\n",
    "\n",
    "def dist(x, y):\n",
    "\treturn math.sqrt(\n",
    "\t\t(x[0]-y[0])**2+\n",
    "\t\t(x[1]-y[1])**2+\n",
    "\t\t(x[2]-y[2])**2\n",
    "\t\t)\n",
    "\n",
    "def place_sphere(matrix, origin, r):\n",
    "\tfor index, x in np.ndenumerate(matrix):\n",
    "\t    if dist(index, origin) <= r:\n",
    "\t    \tmatrix[index] = 1\n",
    "\treturn matrix\n",
    "\n",
    "def random_shapes(m_size,num):\n",
    "    shapes = np.zeros((num,m_size,m_size,m_size))\n",
    "    shape_types = np.random.randint(low = 0,high = 2,size= num)\n",
    "    \n",
    "    for i in range(num):\n",
    "        r = np.random.randint(low = 3,high = np.floor(m_size/2-1))\n",
    "        org = np.random.randint(low = r + 1,high = m_size - r - 1, size = 3)\n",
    "        if shape_types[i] == 0:\n",
    "            shapes[i,:,:,:] = place_cube(shapes[i,:,:,:],org,r)\n",
    "        elif shape_types[i] == 1:\n",
    "            shapes[i,:,:,:] = place_sphere(shapes[i,:,:,:],org,r)\n",
    "            \n",
    "    return shapes, shape_types\n",
    "        \n",
    "\n",
    "def matching_shapes(m_size, num):\n",
    "\tcubes = np.zeros((m_size,m_size,m_size,num))\n",
    "\tspheres = np.zeros((m_size,m_size,m_size,num))\n",
    "\tfor i in range(num):\n",
    "\t\tr = np.random.randint(low = 3,high = np.floor(m_size/2-1))\n",
    "\t\torg = np.random.randint(low = r + 1,high = m_size - r - 1, size = 3)\n",
    "\t\tcubes[:,:,:,i] = place_cube(cubes[:,:,:,i],org,r)\n",
    "\t\tspheres[:,:,:,i] = place_sphere(spheres[:,:,:,i],org,r)\n",
    "\treturn cubes, spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(features, dataconfig):\n",
    "     # Input Layer\n",
    "    # input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
    "    prev_layer = features\n",
    "\n",
    "    in_filters = dataconfig.num_props\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        out_filters = 16\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "        prev_layer = conv1\n",
    "        in_filters = out_filters\n",
    "\n",
    "    pool1 = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "    norm1 = pool1  # tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta = 0.75, name='norm1')\n",
    "\n",
    "    prev_layer = norm1\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        out_filters = 32\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "        prev_layer = conv2\n",
    "        in_filters = out_filters\n",
    "\n",
    "    # normalize prev_layer here\n",
    "    prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('conv3_1') as scope:\n",
    "        out_filters = 64\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    with tf.variable_scope('conv3_2') as scope:\n",
    "        out_filters = 64\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    with tf.variable_scope('conv3_3') as scope:\n",
    "        out_filters = 32\n",
    "        kernel = _weight_variable('weights', [5, 5, 5, in_filters, out_filters])\n",
    "        conv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding='SAME')\n",
    "        biases = _bias_variable('biases', [out_filters])\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        prev_layer = tf.nn.relu(bias, name=scope.name)\n",
    "        in_filters = out_filters\n",
    "\n",
    "    # normalize prev_layer here\n",
    "    prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        prev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n",
    "        weights = _weight_variable('weights', [dim, FC_SIZE])\n",
    "        biases = _bias_variable('biases', [FC_SIZE])\n",
    "        local3 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    prev_layer = local3\n",
    "\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        prev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n",
    "        weights = _weight_variable('weights', [dim, FC_SIZE])\n",
    "        biases = _bias_variable('biases', [FC_SIZE])\n",
    "        local4 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    prev_layer = local4\n",
    "\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        dim = np.prod(prev_layer.get_shape().as_list()[1:])\n",
    "        weights = _weight_variable('weights', [dim, dataconfig.num_classes])\n",
    "        biases = _bias_variable('biases', [dataconfig.num_classes])\n",
    "        softmax_linear = tf.add(tf.matmul(prev_layer, weights), biases, name=scope.name)\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  #\"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "    dataconfig.num_props = 1\n",
    "    dataconfig.num_classes = 2\n",
    "\n",
    "    #input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
    "\n",
    "  # Logits Layer\n",
    "    logits = inference(features, dataconfig)\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "        loss = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=0.001,\n",
    "            optimizer=\"SGD\")\n",
    "\n",
    "  # Generate Predictions\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(\n",
    "          input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(\n",
    "          logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "  # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "        mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Load training and eval data\n",
    "#     mnist = learn.datasets.load_dataset(\"mnist\")\n",
    "#     train_data = mnist.train.images # Returns np.array\n",
    "#     train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "#     eval_data = mnist.test.images # Returns np.array\n",
    "#     eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "    train_data = np.load('rand_shapes_data.npy')\n",
    "    train_labels = np.load('rand_shapes_labels.npy')\n",
    "    \n",
    "    eval_data = np.load('rand_shapes_data_test.npy')\n",
    "    eval_labels = np.laod('rand_shapes_labels_test.npy')\n",
    "    \n",
    "    shapes_classifier = learn.SKCompat(learn.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"shapes_convnet_model\"))\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "        # Train the model\n",
    "    shapes_classifier.fit(\n",
    "        x=train_data,\n",
    "        y=train_labels,\n",
    "        batch_size=100,\n",
    "        steps=1000,\n",
    "        monitors=[logging_hook])\n",
    "    \n",
    "        # Configure the accuracy metric for evaluation\n",
    "    metrics = {\n",
    "        \"accuracy\":\n",
    "            learn.MetricSpec(\n",
    "                metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\n",
    "    }\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_results = shapes_classifier.score(\n",
    "        x=eval_data, y=eval_labels, metrics=metrics)\n",
    "    print(eval_results)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.app.run()\n",
    "except:\n",
    "    print(\"Exception traceback:\")\n",
    "    print('-'*60)\n",
    "    traceback.print_exc(file=sys.stdout)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = random_shapes(32,10000)\n",
    "#Binary data\n",
    "np.save('rand_shapes_data.npy', data)\n",
    "np.save('rand_shapes_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = random_shapes(32, 500)\n",
    "#Binary data\n",
    "np.save('rand_shapes_data_test.npy', data)\n",
    "np.save('rand_shapes_labels_test.npy', labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:condaEnvPy2TF1.2]",
   "language": "python",
   "name": "conda-env-condaEnvPy2TF1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
